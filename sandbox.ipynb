{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f765a9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/workspace/VLM_Embed/src/model/vlm_backbone/internvideo2/modeling_internvideo2.py:541: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FusedMLP of flash_attn is not installed!!!\n",
      "DropoutAddRMSNorm of flash_attn is not installed!!!\n",
      "flash_attn_interface or bert_padding of flash_attn is not installed!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-01 10:34:20,262] DEBUG [matplotlib:342] matplotlib data path: /workspace/VLM_Embed/vlm/lib/python3.11/site-packages/matplotlib/mpl-data\n",
      "[2026-01-01 10:34:20,271] DEBUG [matplotlib:342] CONFIGDIR=/root/.config/matplotlib\n",
      "[2026-01-01 10:34:20,284] DEBUG [matplotlib:1560] interactive is False\n",
      "[2026-01-01 10:34:20,286] DEBUG [matplotlib:1561] platform is linux\n",
      "[2026-01-01 10:34:20,317] DEBUG [matplotlib:342] CACHEDIR=/root/.cache/matplotlib\n",
      "[2026-01-01 10:34:20,321] DEBUG [matplotlib.font_manager:1635] Using fontManager instance from /root/.cache/matplotlib/fontlist-v390.json\n",
      "[2026-01-01 10:34:20,707] DEBUG [matplotlib.pyplot:496] Loaded backend module://matplotlib_inline.backend_inline version unknown.\n",
      "[2026-01-01 10:34:20,712] DEBUG [matplotlib.pyplot:496] Loaded backend inline version unknown.\n"
     ]
    }
   ],
   "source": [
    "from src.arguments import ModelArguments, DataArguments\n",
    "from src.model.model import MMEBModel\n",
    "from src.model.processor import load_processor, QWEN2_VL, VLM_IMAGE_TOKENS, \\\n",
    "    Qwen2_VL_process_fn, LLAVA_QWEN2, FastVLM_process_fn\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers.image_transforms import (\n",
    "    convert_to_rgb,\n",
    "    resize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d924ae36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-01 10:34:20,736] INFO [src.utils:21] Loading processor from: TIGER-Lab/VLM2Vec-Qwen2VL-2B\n",
      "[2026-01-01 10:34:20,758] DEBUG [urllib3.connectionpool:1049] Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Qwen2-VL processor\n",
      ">>>>>>>>>>>>>>>>>>>>>>>> Processor TIGER-Lab/VLM2Vec-Qwen2VL-2B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-01 10:34:23,190] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"GET /api/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/revision/main HTTP/1.1\" 200 6640\n",
      "[2026-01-01 10:34:23,205] DEBUG [urllib3.connectionpool:1049] Starting new HTTPS connection (1): huggingface.co:443\n",
      "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s][2026-01-01 10:34:26,742] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/preprocessor_config.json HTTP/1.1\" 307 0\n",
      "[2026-01-01 10:34:26,951] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "[2026-01-01 10:34:26,955] DEBUG [filelock:331] Attempting to acquire lock 134566049429840 on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/2ef5e95d7d22f4b301fbf6bd6de694f5918a33fd.lock\n",
      "[2026-01-01 10:34:26,957] DEBUG [filelock:334] Lock 134566049429840 acquired on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/2ef5e95d7d22f4b301fbf6bd6de694f5918a33fd.lock\n",
      "[2026-01-01 10:34:27,958] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"GET /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/preprocessor_config.json HTTP/1.1\" 200 567\n",
      "[2026-01-01 10:34:27,965] DEBUG [filelock:364] Attempting to release lock 134566049429840 on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/2ef5e95d7d22f4b301fbf6bd6de694f5918a33fd.lock\n",
      "[2026-01-01 10:34:27,967] DEBUG [filelock:367] Lock 134566049429840 released on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/2ef5e95d7d22f4b301fbf6bd6de694f5918a33fd.lock\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:04<00:00,  4.76s/it]\n",
      "[2026-01-01 10:34:28,270] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/tokenizer_config.json HTTP/1.1\" 307 0\n",
      "[2026-01-01 10:34:28,479] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "[2026-01-01 10:34:28,483] DEBUG [filelock:331] Attempting to acquire lock 134566049422160 on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/84c1ef8e03a2a77a2a0682820d0ec7c37e686312.lock\n",
      "[2026-01-01 10:34:28,485] DEBUG [filelock:334] Lock 134566049422160 acquired on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/84c1ef8e03a2a77a2a0682820d0ec7c37e686312.lock\n",
      "[2026-01-01 10:34:28,717] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"GET /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/tokenizer_config.json HTTP/1.1\" 200 None\n",
      "[2026-01-01 10:34:28,721] DEBUG [filelock:364] Attempting to release lock 134566049422160 on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/84c1ef8e03a2a77a2a0682820d0ec7c37e686312.lock\n",
      "[2026-01-01 10:34:28,722] DEBUG [filelock:367] Lock 134566049422160 released on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/84c1ef8e03a2a77a2a0682820d0ec7c37e686312.lock\n",
      "[2026-01-01 10:34:29,022] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"GET /api/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n",
      "[2026-01-01 10:34:29,428] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/vocab.json HTTP/1.1\" 307 0\n",
      "[2026-01-01 10:34:29,662] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/vocab.json HTTP/1.1\" 200 0\n",
      "[2026-01-01 10:34:29,665] DEBUG [filelock:331] Attempting to acquire lock 134566049861648 on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/4783fe10ac3adce15ac8f358ef5462739852c569.lock\n",
      "[2026-01-01 10:34:29,668] DEBUG [filelock:334] Lock 134566049861648 acquired on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/4783fe10ac3adce15ac8f358ef5462739852c569.lock\n",
      "[2026-01-01 10:34:30,219] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"GET /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/vocab.json HTTP/1.1\" 200 None\n",
      "[2026-01-01 10:34:33,209] DEBUG [filelock:364] Attempting to release lock 134566049861648 on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/4783fe10ac3adce15ac8f358ef5462739852c569.lock\n",
      "[2026-01-01 10:34:33,211] DEBUG [filelock:367] Lock 134566049861648 released on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/4783fe10ac3adce15ac8f358ef5462739852c569.lock\n",
      "[2026-01-01 10:34:33,870] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/merges.txt HTTP/1.1\" 307 0\n",
      "[2026-01-01 10:34:34,105] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/merges.txt HTTP/1.1\" 200 0\n",
      "[2026-01-01 10:34:34,108] DEBUG [filelock:331] Attempting to acquire lock 134566050787216 on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/31349551d90c7606f325fe0f11bbb8bd5fa0d7c7.lock\n",
      "[2026-01-01 10:34:34,110] DEBUG [filelock:334] Lock 134566050787216 acquired on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/31349551d90c7606f325fe0f11bbb8bd5fa0d7c7.lock\n",
      "[2026-01-01 10:34:34,787] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"GET /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/merges.txt HTTP/1.1\" 200 None\n",
      "[2026-01-01 10:34:35,021] DEBUG [filelock:364] Attempting to release lock 134566050787216 on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/31349551d90c7606f325fe0f11bbb8bd5fa0d7c7.lock\n",
      "[2026-01-01 10:34:35,023] DEBUG [filelock:367] Lock 134566050787216 released on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/31349551d90c7606f325fe0f11bbb8bd5fa0d7c7.lock\n",
      "[2026-01-01 10:34:35,309] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/tokenizer.json HTTP/1.1\" 302 0\n",
      "[2026-01-01 10:34:35,313] DEBUG [filelock:331] Attempting to acquire lock 134566049764688 on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/f9f1b25be0b5a53ffc83ca52290a5ebcf3e45ce4ea9fd378dc6d9091bf111ac2.lock\n",
      "[2026-01-01 10:34:35,314] DEBUG [filelock:334] Lock 134566049764688 acquired on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/f9f1b25be0b5a53ffc83ca52290a5ebcf3e45ce4ea9fd378dc6d9091bf111ac2.lock\n",
      "[2026-01-01 10:34:35,605] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"GET /api/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/xet-read-token/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e HTTP/1.1\" 200 365\n",
      "[2026-01-01 10:35:07,526] DEBUG [filelock:364] Attempting to release lock 134566049764688 on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/f9f1b25be0b5a53ffc83ca52290a5ebcf3e45ce4ea9fd378dc6d9091bf111ac2.lock\n",
      "[2026-01-01 10:35:07,528] DEBUG [filelock:367] Lock 134566049764688 released on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/f9f1b25be0b5a53ffc83ca52290a5ebcf3e45ce4ea9fd378dc6d9091bf111ac2.lock\n",
      "[2026-01-01 10:35:08,914] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/added_tokens.json HTTP/1.1\" 307 0\n",
      "[2026-01-01 10:35:09,149] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/added_tokens.json HTTP/1.1\" 200 0\n",
      "[2026-01-01 10:35:09,153] DEBUG [filelock:331] Attempting to acquire lock 134566031113744 on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/caa81304af029feb531f1fe80a096dc539ea0153.lock\n",
      "[2026-01-01 10:35:09,154] DEBUG [filelock:334] Lock 134566031113744 acquired on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/caa81304af029feb531f1fe80a096dc539ea0153.lock\n",
      "[2026-01-01 10:35:09,453] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"GET /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/added_tokens.json HTTP/1.1\" 200 392\n",
      "[2026-01-01 10:35:09,456] DEBUG [filelock:364] Attempting to release lock 134566031113744 on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/caa81304af029feb531f1fe80a096dc539ea0153.lock\n",
      "[2026-01-01 10:35:09,457] DEBUG [filelock:367] Lock 134566031113744 released on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/caa81304af029feb531f1fe80a096dc539ea0153.lock\n",
      "[2026-01-01 10:35:09,748] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/special_tokens_map.json HTTP/1.1\" 307 0\n",
      "[2026-01-01 10:35:09,982] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/special_tokens_map.json HTTP/1.1\" 200 0\n",
      "[2026-01-01 10:35:09,985] DEBUG [filelock:331] Attempting to acquire lock 134566049769168 on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/ac23c0aaa2434523c494330aeb79c58395378103.lock\n",
      "[2026-01-01 10:35:09,987] DEBUG [filelock:334] Lock 134566049769168 acquired on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/ac23c0aaa2434523c494330aeb79c58395378103.lock\n",
      "[2026-01-01 10:35:10,284] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"GET /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/special_tokens_map.json HTTP/1.1\" 200 613\n",
      "[2026-01-01 10:35:10,288] DEBUG [filelock:364] Attempting to release lock 134566049769168 on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/ac23c0aaa2434523c494330aeb79c58395378103.lock\n",
      "[2026-01-01 10:35:10,289] DEBUG [filelock:367] Lock 134566049769168 released on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/ac23c0aaa2434523c494330aeb79c58395378103.lock\n",
      "[2026-01-01 10:35:10,997] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/chat_template.jinja HTTP/1.1\" 404 0\n",
      "[2026-01-01 10:35:13,184] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"GET /api/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/revision/main HTTP/1.1\" 200 6640\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 3483.64it/s]\n",
      "[2026-01-01 10:35:13,607] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/tokenizer_config.json HTTP/1.1\" 307 0\n",
      "[2026-01-01 10:35:13,816] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "[2026-01-01 10:35:14,101] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"GET /api/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n",
      "[2026-01-01 10:35:17,368] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"GET /api/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n",
      "[2026-01-01 10:35:17,677] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/processor_config.json HTTP/1.1\" 404 0\n",
      "[2026-01-01 10:35:18,093] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/chat_template.json HTTP/1.1\" 307 0\n",
      "[2026-01-01 10:35:18,305] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/chat_template.json HTTP/1.1\" 200 0\n",
      "[2026-01-01 10:35:18,314] DEBUG [filelock:331] Attempting to acquire lock 134566049773008 on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/13303be6f396b038cf397b34a8096819d403836b.lock\n",
      "[2026-01-01 10:35:18,318] DEBUG [filelock:334] Lock 134566049773008 acquired on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/13303be6f396b038cf397b34a8096819d403836b.lock\n",
      "[2026-01-01 10:35:18,711] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"GET /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/chat_template.json HTTP/1.1\" 200 None\n",
      "[2026-01-01 10:35:18,716] DEBUG [filelock:364] Attempting to release lock 134566049773008 on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/13303be6f396b038cf397b34a8096819d403836b.lock\n",
      "[2026-01-01 10:35:18,717] DEBUG [filelock:367] Lock 134566049773008 released on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/13303be6f396b038cf397b34a8096819d403836b.lock\n",
      "[2026-01-01 10:35:19,003] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/chat_template.jinja HTTP/1.1\" 404 0\n",
      "[2026-01-01 10:35:19,297] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/audio_tokenizer_config.json HTTP/1.1\" 404 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageProcessor type: <class 'transformers.models.qwen2_vl.image_processing_qwen2_vl.Qwen2VLImageProcessor'>\n",
      "teacher processor loaded here.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-01 10:35:21,217] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /TIGER-Lab/VLM2Vec-Qwen2VL-2B/resolve/main/config.json HTTP/1.1\" 307 0\n",
      "[2026-01-01 10:35:21,428] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/config.json HTTP/1.1\" 200 0\n",
      "[2026-01-01 10:35:21,436] DEBUG [filelock:331] Attempting to acquire lock 134566049686608 on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/85b97e98aba8ab3f53cefd874cf7b6c574e3221c.lock\n",
      "[2026-01-01 10:35:21,440] DEBUG [filelock:334] Lock 134566049686608 acquired on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/85b97e98aba8ab3f53cefd874cf7b6c574e3221c.lock\n",
      "[2026-01-01 10:35:22,357] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"GET /api/resolve-cache/models/TIGER-Lab/VLM2Vec-Qwen2VL-2B/7717deedf0631e6f520b7c83c8f82dcbc2c4c21e/config.json HTTP/1.1\" 200 None\n",
      "[2026-01-01 10:35:22,362] DEBUG [filelock:364] Attempting to release lock 134566049686608 on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/85b97e98aba8ab3f53cefd874cf7b6c574e3221c.lock\n",
      "[2026-01-01 10:35:22,364] DEBUG [filelock:367] Lock 134566049686608 released on /root/.cache/huggingface/hub/.locks/models--TIGER-Lab--VLM2Vec-Qwen2VL-2B/85b97e98aba8ab3f53cefd874cf7b6c574e3221c.lock\n",
      "[2026-01-01 10:35:22,377] INFO [src.utils:21] Loading backbone [qwen2_vl] from TIGER-Lab/VLM2Vec-Qwen2VL-2B\n",
      "[2026-01-01 10:35:22,378] INFO [src.utils:21] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected model type: qwen2_vl\n",
      "Determined model backbone: qwen2_vl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-01 10:35:22,757] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /Qwen/Qwen2-VL-2B/resolve/main/config.json HTTP/1.1\" 307 0\n",
      "[2026-01-01 10:35:22,966] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/Qwen/Qwen2-VL-2B/d3a53f2484fce9d62fff115a5ddfc833f873bfde/config.json HTTP/1.1\" 200 0\n",
      "[2026-01-01 10:35:22,971] DEBUG [filelock:331] Attempting to acquire lock 134566059150416 on /root/.cache/huggingface/hub/.locks/models--Qwen--Qwen2-VL-2B/f74c2d569aa6de68f4c3c70d427ca2eeb9ab802c.lock\n",
      "[2026-01-01 10:35:22,973] DEBUG [filelock:334] Lock 134566059150416 acquired on /root/.cache/huggingface/hub/.locks/models--Qwen--Qwen2-VL-2B/f74c2d569aa6de68f4c3c70d427ca2eeb9ab802c.lock\n",
      "[2026-01-01 10:35:23,250] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"GET /api/resolve-cache/models/Qwen/Qwen2-VL-2B/d3a53f2484fce9d62fff115a5ddfc833f873bfde/config.json HTTP/1.1\" 200 None\n",
      "[2026-01-01 10:35:23,256] DEBUG [filelock:364] Attempting to release lock 134566059150416 on /root/.cache/huggingface/hub/.locks/models--Qwen--Qwen2-VL-2B/f74c2d569aa6de68f4c3c70d427ca2eeb9ab802c.lock\n",
      "[2026-01-01 10:35:23,258] DEBUG [filelock:367] Lock 134566059150416 released on /root/.cache/huggingface/hub/.locks/models--Qwen--Qwen2-VL-2B/f74c2d569aa6de68f4c3c70d427ca2eeb9ab802c.lock\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "[2026-01-01 10:35:23,668] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /Qwen/Qwen2-VL-2B/resolve/main/adapter_config.json HTTP/1.1\" 404 0\n",
      "[2026-01-01 10:35:23,975] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /Qwen/Qwen2-VL-2B/resolve/main/model.safetensors HTTP/1.1\" 404 0\n",
      "[2026-01-01 10:35:24,530] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /Qwen/Qwen2-VL-2B/resolve/main/model.safetensors.index.json HTTP/1.1\" 307 0\n",
      "[2026-01-01 10:35:25,301] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/Qwen/Qwen2-VL-2B/d3a53f2484fce9d62fff115a5ddfc833f873bfde/model.safetensors.index.json HTTP/1.1\" 200 0\n",
      "[2026-01-01 10:35:25,306] DEBUG [filelock:331] Attempting to acquire lock 134566049766992 on /root/.cache/huggingface/hub/.locks/models--Qwen--Qwen2-VL-2B/293d54cd8408e82d1bdd2eabd07d9311eb406b52.lock\n",
      "[2026-01-01 10:35:25,308] DEBUG [filelock:334] Lock 134566049766992 acquired on /root/.cache/huggingface/hub/.locks/models--Qwen--Qwen2-VL-2B/293d54cd8408e82d1bdd2eabd07d9311eb406b52.lock\n",
      "[2026-01-01 10:35:25,716] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"GET /api/resolve-cache/models/Qwen/Qwen2-VL-2B/d3a53f2484fce9d62fff115a5ddfc833f873bfde/model.safetensors.index.json HTTP/1.1\" 200 None\n",
      "[2026-01-01 10:35:25,720] DEBUG [filelock:364] Attempting to release lock 134566049766992 on /root/.cache/huggingface/hub/.locks/models--Qwen--Qwen2-VL-2B/293d54cd8408e82d1bdd2eabd07d9311eb406b52.lock\n",
      "[2026-01-01 10:35:25,722] DEBUG [filelock:367] Lock 134566049766992 released on /root/.cache/huggingface/hub/.locks/models--Qwen--Qwen2-VL-2B/293d54cd8408e82d1bdd2eabd07d9311eb406b52.lock\n",
      "[2026-01-01 10:35:26,236] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"GET /api/models/Qwen/Qwen2-VL-2B/revision/main HTTP/1.1\" 200 2756\n",
      "[2026-01-01 10:35:26,246] DEBUG [urllib3.connectionpool:1049] Starting new HTTPS connection (1): huggingface.co:443\n",
      "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s][2026-01-01 10:35:26,252] DEBUG [urllib3.connectionpool:1049] Starting new HTTPS connection (1): huggingface.co:443\n",
      "[2026-01-01 10:35:28,590] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /Qwen/Qwen2-VL-2B/resolve/d3a53f2484fce9d62fff115a5ddfc833f873bfde/model-00001-of-00002.safetensors HTTP/1.1\" 302 0\n",
      "[2026-01-01 10:35:28,597] DEBUG [filelock:331] Attempting to acquire lock 134566049857616 on /root/.cache/huggingface/hub/.locks/models--Qwen--Qwen2-VL-2B/be54458131380b266222f11f0fec1704f85d6695b226d333891c916478306c1a.lock\n",
      "[2026-01-01 10:35:28,599] DEBUG [filelock:334] Lock 134566049857616 acquired on /root/.cache/huggingface/hub/.locks/models--Qwen--Qwen2-VL-2B/be54458131380b266222f11f0fec1704f85d6695b226d333891c916478306c1a.lock\n",
      "[2026-01-01 10:35:29,497] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"GET /api/models/Qwen/Qwen2-VL-2B/xet-read-token/d3a53f2484fce9d62fff115a5ddfc833f873bfde HTTP/1.1\" 200 365\n",
      "[2026-01-01 10:35:29,580] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"HEAD /Qwen/Qwen2-VL-2B/resolve/d3a53f2484fce9d62fff115a5ddfc833f873bfde/model-00002-of-00002.safetensors HTTP/1.1\" 302 0\n",
      "[2026-01-01 10:35:29,584] DEBUG [filelock:331] Attempting to acquire lock 134566031129808 on /root/.cache/huggingface/hub/.locks/models--Qwen--Qwen2-VL-2B/b1e5ceb96ab4b2660156d6429b70e1dfcdee8570962ed921aadfc0a470a8e738.lock\n",
      "[2026-01-01 10:35:29,586] DEBUG [filelock:334] Lock 134566031129808 acquired on /root/.cache/huggingface/hub/.locks/models--Qwen--Qwen2-VL-2B/b1e5ceb96ab4b2660156d6429b70e1dfcdee8570962ed921aadfc0a470a8e738.lock\n",
      "[2026-01-01 10:35:29,858] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"GET /api/models/Qwen/Qwen2-VL-2B/xet-read-token/d3a53f2484fce9d62fff115a5ddfc833f873bfde HTTP/1.1\" 200 365\n",
      "[2026-01-01 10:53:29,804] DEBUG [urllib3.connectionpool:1049] Starting new HTTPS connection (1): huggingface.co:443\n",
      "[2026-01-01 10:53:31,654] DEBUG [urllib3.connectionpool:544] https://huggingface.co:443 \"GET /api/models/Qwen/Qwen2-VL-2B/xet-read-token/d3a53f2484fce9d62fff115a5ddfc833f873bfde HTTP/1.1\" 200 365\n",
      "Cancellation requested; stopping current tasks.\n",
      "[2026-01-01 10:55:34,930] DEBUG [filelock:364] Attempting to release lock 134566031129808 on /root/.cache/huggingface/hub/.locks/models--Qwen--Qwen2-VL-2B/b1e5ceb96ab4b2660156d6429b70e1dfcdee8570962ed921aadfc0a470a8e738.lock\n",
      "[2026-01-01 10:55:34,932] DEBUG [filelock:364] Attempting to release lock 134566049857616 on /root/.cache/huggingface/hub/.locks/models--Qwen--Qwen2-VL-2B/be54458131380b266222f11f0fec1704f85d6695b226d333891c916478306c1a.lock\n",
      "[2026-01-01 10:55:34,932] DEBUG [filelock:367] Lock 134566031129808 released on /root/.cache/huggingface/hub/.locks/models--Qwen--Qwen2-VL-2B/b1e5ceb96ab4b2660156d6429b70e1dfcdee8570962ed921aadfc0a470a8e738.lock\n",
      "[2026-01-01 10:55:34,935] DEBUG [filelock:367] Lock 134566049857616 released on /root/.cache/huggingface/hub/.locks/models--Qwen--Qwen2-VL-2B/be54458131380b266222f11f0fec1704f85d6695b226d333891c916478306c1a.lock\n",
      "Fetching 2 files:   0%|          | 0/2 [20:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m data_args = DataArguments()\n\u001b[32m     11\u001b[39m processor = load_processor(model_args, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m model = \u001b[43mMMEBModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m model = model.to(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m, dtype=torch.bfloat16)\n\u001b[32m     14\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/src/model/model.py:375\u001b[39m, in \u001b[36mMMEBModel.load\u001b[39m\u001b[34m(cls, model_args, is_trainable, **kwargs)\u001b[39m\n\u001b[32m    373\u001b[39m     config._attn_implementation = \u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    374\u001b[39m     config.vision_config._attn_implementation = \u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     base_model = \u001b[43mbackbone2model\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_backbone\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m model_args.model_backbone \u001b[38;5;129;01min\u001b[39;00m [INTERN_VL3]:\n\u001b[32m    382\u001b[39m     config._attn_implementation = \u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/transformers/modeling_utils.py:288\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    290\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/transformers/modeling_utils.py:5027\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5017\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   5018\u001b[39m     gguf_file\n\u001b[32m   5019\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   5020\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map.values()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[32m   5021\u001b[39m ):\n\u001b[32m   5022\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   5023\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5024\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded from GGUF files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5025\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m5027\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5028\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5029\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5031\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5032\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5033\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5034\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5035\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5036\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5037\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5038\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5039\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5040\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5041\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5042\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5043\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   5044\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5045\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5047\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   5048\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/transformers/modeling_utils.py:1308\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[39m\n\u001b[32m   1306\u001b[39m sharded_metadata = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1307\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[32m-> \u001b[39m\u001b[32m1308\u001b[39m     checkpoint_files, sharded_metadata = \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1319\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1322\u001b[39m     checkpoint_files = [resolved_archive_file] \u001b[38;5;28;01mif\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/transformers/utils/hub.py:1119\u001b[39m, in \u001b[36mget_checkpoint_shard_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m shard_filenames, sharded_metadata\n\u001b[32m   1117\u001b[39m \u001b[38;5;66;03m# At this stage pretrained_model_name_or_path is a model identifier on the Hub. Try to get everything from cache,\u001b[39;00m\n\u001b[32m   1118\u001b[39m \u001b[38;5;66;03m# or download the files\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1119\u001b[39m cached_filenames = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshard_filenames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cached_filenames, sharded_metadata\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/transformers/utils/hub.py:493\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m         hf_hub_download(\n\u001b[32m    479\u001b[39m             path_or_repo_id,\n\u001b[32m    480\u001b[39m             filenames[\u001b[32m0\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    490\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    491\u001b[39m         )\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m         \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_filenames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    508\u001b[39m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[32m    509\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/huggingface_hub/_snapshot_download.py:332\u001b[39m, in \u001b[36msnapshot_download\u001b[39m\u001b[34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[39m\n\u001b[32m    330\u001b[39m         _inner_hf_hub_download(file)\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_inner_hf_hub_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# User can use its own tqdm class or the default one from `huggingface_hub.utils`\u001b[39;49;00m\n\u001b[32m    338\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(os.path.realpath(local_dir))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/tqdm/contrib/concurrent.py:69\u001b[39m, in \u001b[36mthread_map\u001b[39m\u001b[34m(fn, *iterables, **tqdm_kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m \u001b[33;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconcurrent\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfutures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/tqdm/contrib/concurrent.py:51\u001b[39m, in \u001b[36m_executor_map\u001b[39m\u001b[34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name=lock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers=max_workers, initializer=tqdm_class.set_lock,\n\u001b[32m     50\u001b[39m                       initargs=(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/concurrent/futures/_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/threading.py:327\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model_args = ModelArguments(\n",
    "    model_name='Qwen/Qwen2-VL-2B',\n",
    "    checkpoint_path='TIGER-Lab/VLM2Vec-Qwen2VL-2B',\n",
    "    pooling='last',\n",
    "    normalize=True,\n",
    "    model_backbone='qwen2_vl',\n",
    "    lora=True\n",
    ")\n",
    "data_args = DataArguments()\n",
    "\n",
    "processor = load_processor(model_args, None)\n",
    "model = MMEBModel.load(model_args)\n",
    "model = model.to('cuda', dtype=torch.bfloat16)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e449f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llava_qwen2'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model_backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "629ef9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaConfig {\n",
       "  \"architectures\": [\n",
       "    \"LlavaQwen2ForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"auto_map\": {\n",
       "    \"AutoConfig\": \"llava_qwen.LlavaConfig\",\n",
       "    \"AutoModelForCausalLM\": \"llava_qwen.LlavaQwen2ForCausalLM\"\n",
       "  },\n",
       "  \"bos_token_id\": 151643,\n",
       "  \"dtype\": \"bfloat16\",\n",
       "  \"eos_token_id\": 151645,\n",
       "  \"freeze_mm_mlp_adapter\": false,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 896,\n",
       "  \"image_aspect_ratio\": \"pad\",\n",
       "  \"image_grid_pinpoints\": null,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4864,\n",
       "  \"layer_types\": [\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\"\n",
       "  ],\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"max_window_layers\": 24,\n",
       "  \"mm_hidden_size\": 3072,\n",
       "  \"mm_patch_merge_type\": \"flat\",\n",
       "  \"mm_projector_lr\": null,\n",
       "  \"mm_projector_type\": \"mlp2x_gelu\",\n",
       "  \"mm_use_im_patch_token\": false,\n",
       "  \"mm_use_im_start_end\": false,\n",
       "  \"mm_vision_select_feature\": \"patch\",\n",
       "  \"mm_vision_select_layer\": -2,\n",
       "  \"mm_vision_tower\": \"mobileclip_l_1024\",\n",
       "  \"model_type\": \"llava_qwen2\",\n",
       "  \"num_attention_heads\": 14,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"num_key_value_heads\": 2,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 1000000.0,\n",
       "  \"sliding_window\": null,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"tokenizer_model_max_length\": 8192,\n",
       "  \"tokenizer_padding_side\": \"right\",\n",
       "  \"transformers_version\": \"4.56.1\",\n",
       "  \"tune_mm_mlp_adapter\": false,\n",
       "  \"unfreeze_mm_vision_tower\": true,\n",
       "  \"use_cache\": true,\n",
       "  \"use_mm_proj\": true,\n",
       "  \"use_sliding_window\": false,\n",
       "  \"vocab_size\": 151936\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ecddd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_tower = model.encoder.get_vision_tower()\n",
    "vision_tower.config['image_cfg']['patch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab11ed4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (3072x12x12). Calculated output size: (3072x0x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m x = torch.randn(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m768\u001b[39m, \u001b[32m768\u001b[39m).to(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m, dtype=torch.bfloat16)\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     y = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_vision_tower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m y.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/torch/nn/modules/module.py:1780\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1779\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1780\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/torch/nn/modules/module.py:1791\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1786\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1787\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1789\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1790\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1791\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1793\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1794\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/src/model/llava/model/multimodal_encoder/mobileclip_encoder.py:75\u001b[39m, in \u001b[36mMobileCLIPVisionTower.forward\u001b[39m\u001b[34m(self, images)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images):\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tune_vision_tower:\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/src/model/llava/model/multimodal_encoder/mobileclip_encoder.py:88\u001b[39m, in \u001b[36mMobileCLIPVisionTower.forward_images\u001b[39m\u001b[34m(self, images)\u001b[39m\n\u001b[32m     86\u001b[39m         image_features.append(image_feature)\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     image_forward_outs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvision_tower\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_image_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     image_features = \u001b[38;5;28mself\u001b[39m.feature_select(image_forward_outs).to(images.dtype)\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m image_features\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/torch/nn/modules/module.py:1780\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1779\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1780\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/torch/nn/modules/module.py:1791\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1786\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1787\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1789\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1790\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1791\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1793\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1794\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/src/model/llava/model/multimodal_encoder/mobileclip/__init__.py:57\u001b[39m, in \u001b[36mMCi.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Any, *args, **kwargs) -> Any:\n\u001b[32m     56\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"A forward function of the model.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/torch/nn/modules/module.py:1780\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1779\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1780\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/torch/nn/modules/module.py:1791\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1786\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1787\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1789\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1790\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1791\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1793\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1794\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/apple/FastVLM-0.5B/16375720c2d673fa583e57e9876afde27549c7d0/llava_qwen.py:1446\u001b[39m, in \u001b[36mFastViT.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m   1444\u001b[39m x = \u001b[38;5;28mself\u001b[39m.forward_tokens(x)\n\u001b[32m   1445\u001b[39m \u001b[38;5;66;03m# for image classification/embedding\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1446\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1447\u001b[39m cls_out = \u001b[38;5;28mself\u001b[39m.head(x)\n\u001b[32m   1449\u001b[39m out_dict = \u001b[38;5;28mdict\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/torch/nn/modules/module.py:1780\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1779\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1780\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/torch/nn/modules/module.py:1791\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1786\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1787\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1789\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1790\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1791\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1793\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1794\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/apple/FastVLM-0.5B/16375720c2d673fa583e57e9876afde27549c7d0/llava_qwen.py:232\u001b[39m, in \u001b[36mMobileOneBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[38;5;66;03m# Inference mode forward pass.\u001b[39;00m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.inference_mode:\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.activation(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreparam_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    234\u001b[39m \u001b[38;5;66;03m# Multi-branched train-time forward pass.\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[38;5;66;03m# Skip branch output\u001b[39;00m\n\u001b[32m    236\u001b[39m identity_out = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/torch/nn/modules/module.py:1780\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1779\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1780\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/VLM_Embed/vlm/lib/python3.11/site-packages/torch/nn/modules/module.py:1791\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1786\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1787\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1789\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1790\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1791\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1793\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1794\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/apple/FastVLM-0.5B/16375720c2d673fa583e57e9876afde27549c7d0/llava_qwen.py:111\u001b[39m, in \u001b[36mSEBlock.forward\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    109\u001b[39m b, c, h, w = inputs.size()\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# x = F.avg_pool2d(inputs, kernel_size=[h, w])\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m x = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mavg_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m x = \u001b[38;5;28mself\u001b[39m.reduce(x)\n\u001b[32m    113\u001b[39m x = F.relu(x)\n",
      "\u001b[31mRuntimeError\u001b[39m: Given input size: (3072x12x12). Calculated output size: (3072x0x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 768, 768).to('cuda', dtype=torch.bfloat16)\n",
    "with torch.no_grad():\n",
    "    y = model.encoder.get_vision_tower()(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b91a84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
